{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face and Eye Tracking Using OPenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face and eye detection using OpenCV typically relies on Haar Cascade Classifiers and is based on the principles of feature extraction, cascading classifiers, and pattern matching. Here's an overview of the principles involved:\n",
    "\n",
    "**1. Feature Extraction:**\n",
    "\n",
    "   - **Haar-like Features**: Haar-like features are simple rectangular filters used to describe the characteristics of an object in an image. They can capture changes in pixel intensity and are used to define the patterns of interest, such as the features of faces and eyes.\n",
    "\n",
    "**2. Cascading Classifiers:**\n",
    "\n",
    "   - **Cascade of Classifiers**: A cascade of classifiers is used to efficiently detect objects in an image at multiple levels of detail. It consists of several stages, where each stage contains a set of weak classifiers (Haar-like feature classifiers).\n",
    "\n",
    "   - **AdaBoost**: The AdaBoost algorithm is employed to select and train the weak classifiers. AdaBoost assigns weights to training samples and focuses on misclassified samples in each iteration, making it an effective way to improve classification performance.\n",
    "\n",
    "   - **Thresholding and Filtering**: At each stage of the cascade, the classifiers examine a region of interest (ROI) in the image. If the ROI passes a threshold (indicating the possibility of an object being present), it is forwarded to the next stage. This approach allows for rapid rejection of non-object regions, reducing the computational load.\n",
    "\n",
    "**3. Pattern Matching:**\n",
    "\n",
    "   - **Sliding Window**: The detection process typically involves sliding a rectangular window over the image at multiple scales and positions. At each position and scale, the Haar-like features are extracted from the region covered by the window.\n",
    "\n",
    "   - **Integral Images**: To efficiently compute Haar-like features, integral images are used. An integral image allows for the fast computation of the sum of pixel values within any rectangular region in constant time.\n",
    "\n",
    "   - **Classifier Response**: The Haar-like features are applied to the integral image, and a classifier response is computed. The response indicates whether the region being examined matches the expected pattern (e.g., a face or an eye).\n",
    "\n",
    "**4. Non-Maximum Suppression:**\n",
    "\n",
    "   - To eliminate duplicate or overlapping detections, a non-maximum suppression algorithm is often applied. It ensures that only the most relevant and significant detections are retained.\n",
    "\n",
    "**5. Training:**\n",
    "\n",
    "   - The Haar Cascade classifiers for face and eye detection are trained on a large dataset containing positive samples (images of faces and eyes) and negative samples (images without faces or eyes). The training process involves creating a strong classifier by combining multiple weak classifiers.\n",
    "\n",
    "**6. Real-Time Processing:**\n",
    "\n",
    "   - Haar Cascade classifiers are efficient and suitable for real-time processing because of the cascading structure, feature extraction, and early rejection of non-object regions.\n",
    "\n",
    "In summary, the principles of face and eye detection using OpenCV involve the use of Haar-like features, cascading classifiers with AdaBoost, pattern matching through sliding windows and integral images, and post-processing steps like non-maximum suppression. These techniques together make it possible to detect faces and eyes in images and video streams with high accuracy and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing eye tracking using OpenCV can be a complex task, and it typically involves multiple steps, including face detection, eye detection, and tracking. Here, I'll provide a simplified example of how to perform basic eye tracking using OpenCV. Keep in mind that for robust and real-time eye tracking applications, you might need more sophisticated techniques and hardware like eye-tracking cameras.\n",
    "\n",
    "In this example, we'll use Haar Cascade Classifiers for face and eye detection and basic tracking of the detected eyes:\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haar Cascade classifiers for face and eye detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Create a video capture object for the default camera (0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale for face and eye detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Perform face detection\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Loop through detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Extract the region of interest (ROI) within the face rectangle\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = frame[y:y + h, x:x + w]\n",
    "        \n",
    "        # Perform eye detection within the face ROI\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        \n",
    "        # Loop through detected eyes\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            # Draw a rectangle around each eye (within the face ROI)\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with detected faces and eyes\n",
    "    cv2.imshow('Eye Tracking', frame)\n",
    "\n",
    "    # Exit the loop if the 'Esc' key is pressed\n",
    "    if cv2.waitKey(30) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "In this code:\n",
    "\n",
    "1. We use Haar Cascade classifiers for both face and eye detection. You need to make sure that the Haar Cascade classifier XML files are available in your OpenCV data directory.\n",
    "\n",
    "2. The code captures video from the webcam, converts frames to grayscale for detection, and performs face detection using `face_cascade.detectMultiScale()`.\n",
    "\n",
    "3. If a face is detected, we extract the region of interest (ROI) within the face rectangle and perform eye detection within this ROI using `eye_cascade.detectMultiScale()`.\n",
    "\n",
    "4. Detected faces and eyes are marked with rectangles on the original frame.\n",
    "\n",
    "5. The frame with detection results is continuously displayed, and the loop exits when the 'Esc' key is pressed.\n",
    "\n",
    "This is a basic example of eye tracking within detected faces. For more accurate and real-time eye tracking, you may need to consider more advanced techniques and algorithms, including optical flow, gaze estimation, and dedicated eye-tracking hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haar Cascade classifiers for face and eye detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Create a video capture object for the default camera (0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale for face and eye detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Perform face detection\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Loop through detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Extract the region of interest (ROI) within the face rectangle\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = frame[y:y + h, x:x + w]\n",
    "        \n",
    "        # Perform eye detection within the face ROI\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        \n",
    "        # Loop through detected eyes\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            # Draw a rectangle around each eye (within the face ROI)\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with detected faces and eyes\n",
    "    cv2.imshow('Eye Tracking', frame)\n",
    "\n",
    "    # Exit the loop if the 'Esc' key is pressed\n",
    "    if cv2.waitKey(30) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "face=cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "eye = cv2.CascadeClassifier('haarcascade_eye.xml') #for detecting eyes\n",
    "def dector(img):\n",
    "    gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face.detectMultiScale(gray,1.3,5)\n",
    "    \n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(127,0,125),3)\n",
    "        \n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        \n",
    "        eyes = eye.detectMultiScale(roi_gray,1.3,3)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.circle(roi_color,(ex+27,ey+27),20,(255,255,0),2)\n",
    "\n",
    "    return img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret,frame =cap.read()\n",
    "    frame = cv2.flip(frame,2)\n",
    "    cv2.imshow(\"face dect\",dector(frame))\n",
    "    if cv2.waitKey(1)==13:   # press enter to terminate\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
